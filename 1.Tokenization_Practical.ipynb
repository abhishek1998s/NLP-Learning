{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC4KURFnpS3l3Vf6gP8GID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishek1998s/NLP-Learning/blob/main/1.Tokenization_Practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYq1lO3cvuNG",
        "outputId": "3b4e7ca6-a354-49b7-f777-0127c9604111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Hello Welcome,to Learn NLP with me.\n",
        "Tokenization is an important NLP task.\n",
        "It helps break down text into smaller units.\"\"\""
      ],
      "metadata": {
        "id": "9SGoOEA4wiqx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWc1FeMTxW1z",
        "outputId": "00217ea3-9038-4fd6-dc83-ec6f95a9604a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to Learn NLP with me.\n",
            "Tokenization is an important NLP task.\n",
            "It helps break down text into smaller units.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##tokenization\n",
        "##step 1: sentence---> paragragh\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_3IrVk0xZdn",
        "outputId": "c9c101b9-6a2d-4610-e4f5-956bfb6fd852"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = sent_tokenize(corpus)\n",
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDW-OIRpypqO",
        "outputId": "3656ff05-0ade-4617-8523-2117bcdf13be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Welcome,to Learn NLP with me.',\n",
              " 'Tokenization is an important NLP task.',\n",
              " 'It helps break down text into smaller units.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuncpxeSywy3",
        "outputId": "e853772b-e39c-4506-b2c2-0c49a91c9056"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in document:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UzLYG77zUze",
        "outputId": "f082a200-5638-4f1d-dadd-0192ae6f2450"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to Learn NLP with me.\n",
            "Tokenization is an important NLP task.\n",
            "It helps break down text into smaller units.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##tokenize\n",
        "##paragraph-----> words\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "FsUQBWDAziJY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSpTQ9mq0B79",
        "outputId": "b821235e-37a4-499e-d556-c64c95ada829"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Learn',\n",
              " 'NLP',\n",
              " 'with',\n",
              " 'me',\n",
              " '.',\n",
              " 'Tokenization',\n",
              " 'is',\n",
              " 'an',\n",
              " 'important',\n",
              " 'NLP',\n",
              " 'task',\n",
              " '.',\n",
              " 'It',\n",
              " 'helps',\n",
              " 'break',\n",
              " 'down',\n",
              " 'text',\n",
              " 'into',\n",
              " 'smaller',\n",
              " 'units',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##sentence------->words\n",
        "for sentence in document:\n",
        "  print(word_tokenize(sentence))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKrG13DW0F6C",
        "outputId": "bbc14bbd-4988-4eb2-ec3e-136ef64d0a28"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'to', 'Learn', 'NLP', 'with', 'me', '.']\n",
            "['Tokenization', 'is', 'an', 'important', 'NLP', 'task', '.']\n",
            "['It', 'helps', 'break', 'down', 'text', 'into', 'smaller', 'units', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##another library to tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "DEO_Xu4A1Ctm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owxb2hDu16Af",
        "outputId": "29ee0dca-4e24-4ec5-88cf-9c838d153a14"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Learn',\n",
              " 'NLP',\n",
              " 'with',\n",
              " 'me',\n",
              " '.',\n",
              " 'Tokenization',\n",
              " 'is',\n",
              " 'an',\n",
              " 'important',\n",
              " 'NLP',\n",
              " 'task',\n",
              " '.',\n",
              " 'It',\n",
              " 'helps',\n",
              " 'break',\n",
              " 'down',\n",
              " 'text',\n",
              " 'into',\n",
              " 'smaller',\n",
              " 'units',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## another method is using tree bank word tokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "token = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "yrZgZjZ41-5F"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmXk_I733O28",
        "outputId": "f7cd17c9-9e77-46fa-c8a2-382ad020d15f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Learn',\n",
              " 'NLP',\n",
              " 'with',\n",
              " 'me.',\n",
              " 'Tokenization',\n",
              " 'is',\n",
              " 'an',\n",
              " 'important',\n",
              " 'NLP',\n",
              " 'task.',\n",
              " 'It',\n",
              " 'helps',\n",
              " 'break',\n",
              " 'down',\n",
              " 'text',\n",
              " 'into',\n",
              " 'smaller',\n",
              " 'units',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7g7KT_yV3Sh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}